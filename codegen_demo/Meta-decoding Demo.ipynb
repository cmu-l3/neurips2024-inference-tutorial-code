{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f51599-188f-40d0-a13f-bb0c360b4179",
   "metadata": {},
   "source": [
    "## Meta-generation case study: Python code generation\n",
    "\n",
    "This notebook will demonstrate __best-of-n__, __self-repair__, and __minimum Bayes risk decoding__ for code generation on the Mostly Basic Python Problems (MBPP) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cd8829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from litellm import completion, ModelResponse\n",
    "import datasets\n",
    "import jellyfish\n",
    "import numpy as np\n",
    "\n",
    "from typing import Callable, Union\n",
    "from pprint import pprint\n",
    "\n",
    "from execute import (\n",
    "    check_correctness,\n",
    "    execute_tests,\n",
    "    execute_codes,\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    make_prompt,\n",
    "    extract_code,\n",
    "    extract_func_calls,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a2285d-ae3d-45bb-b9c9-157bdf63e7ed",
   "metadata": {},
   "source": [
    "First, let's load in the MBPP data. MBPP consists of basic algorithmic Python questions, such as the following:\n",
    "\n",
    "\n",
    "```python\n",
    "# Specification\n",
    "\"\"\"\n",
    "Write a python function to remove first and last occurrence of a given character from the string.\n",
    "\"\"\"\n",
    "\n",
    "# Ground truth function\n",
    "def remove_Occ(s,ch): \n",
    "    for i in range(len(s)): \n",
    "        if (s[i] == ch): \n",
    "            s = s[0 : i] + s[i + 1:] \n",
    "            break\n",
    "    for i in range(len(s) - 1,-1,-1):  \n",
    "        if (s[i] == ch): \n",
    "            s = s[0 : i] + s[i + 1:] \n",
    "            break\n",
    "    return s \n",
    "\n",
    "# Test Cases\n",
    "assert remove_Occ(\"hello\",\"l\") == \"heo\"\n",
    "assert remove_Occ(\"abcda\",\"a\") == \"bcd\"\n",
    "assert remove_Occ(\"PHP\",\"P\") == \"H\"\n",
    "```\n",
    "\n",
    "We'll evaluate outputs based on the execution accuracy on a set of test cases. Specifically, for sampling-based methods, we'll consider the average accuracy across all samples (as there is no ranking over samples), while for meta-decoding methods, we'll consider the accuracy of the top-1 returned output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db06b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mbpp data\n",
    "mbpp = datasets.load_dataset(\"mbpp\", split=\"test\")\n",
    "\n",
    "# pprint(\"MBPP example:\")\n",
    "# pprint(mbpp[0])\n",
    "# print()\n",
    "\n",
    "prompts = []\n",
    "\n",
    "# make zero shot prompts for MBPP\n",
    "for example in mbpp:\n",
    "    prompts.append(make_prompt(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b681268-992e-4adf-a2ff-3ade4fd89a1e",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "\n",
    "First, consider simply sampling a set of outputs from the model, using temperature & top-$p$ sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8836a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for sampling from generator\n",
    "\n",
    "MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "\n",
    "def generate_code(prompt: Union[str, list[dict]], **generate_kwargs) -> tuple[list[str], ModelResponse]:\n",
    "    '''\n",
    "    Generates code sample(s) for prompt\n",
    "    Returns list of code samples and OpenAI-like response object.\n",
    "    \n",
    "    This function accepts additional keyword arguments for various generation \n",
    "    parameters supported by the openai api, including temperature, top_p, logprobs, etc.\n",
    "    '''\n",
    "    if isinstance(prompt, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    else:\n",
    "        assert isinstance(prompt, list) and isinstance(prompt[0], dict)\n",
    "        messages = prompt\n",
    "\n",
    "    response = completion(MODEL_NAME, messages=messages, **generate_kwargs)\n",
    "\n",
    "    # post-processing to extract code from chat model outputs\n",
    "    codes = []\n",
    "    for choice in response.choices:\n",
    "        text = choice.message.content\n",
    "        code = extract_code(text)\n",
    "        codes.append(code)\n",
    "    return codes, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "63e257eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, response = generate_code(prompts[0], n=10, temperature=0.8, top_p=0.95, logprobs=True, top_logprobs=1)\n",
    "\n",
    "assert all(len(c) > 0 for c in codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c2c2e988-876f-429d-ad2f-72cbaea1011d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean pass@1: 0.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution_results = execute_tests(codes, mbpp[0]['test_list'])\n",
    "print(\"mean pass@1:\", sum(result['passed'] for result in execution_results) / len(execution_results))\n",
    "[result['result'] for result in execution_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6215866b",
   "metadata": {},
   "source": [
    "### Best-of-$n$\n",
    "\n",
    "$$\\hat{y} = \\arg \\max_{y \\in \\mathcal{Y}} V(y)$$\n",
    "\n",
    "As our first meta-decoding algorithm, we consider best-of-$n$ using a sample's log probability under the generator as its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74c8508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_of_n_logprob(prompt: str, **generate_kwargs) -> tuple[list[str], list[float]]:\n",
    "    '''\n",
    "    Runs best-of-n generation, using mean sequence logprob as value\n",
    "    Returns list of codes in order of decreasing value\n",
    "    '''\n",
    "    generate_kwargs[\"top_logprobs\"] = 1\n",
    "    generate_kwargs[\"logprobs\"] = True\n",
    "    # generate samples\n",
    "    codes, response = generate_code(prompt, **generate_kwargs)\n",
    "    scores = []\n",
    "    # compute mean logprob for each sequence\n",
    "    for choice in response.choices:\n",
    "        logprobs = choice.logprobs['content']\n",
    "        mean_logprob = sum(lp['logprob'] for lp in logprobs) / len(logprobs)\n",
    "        scores.append(mean_logprob)\n",
    "\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    # arrange codes by decreasing mean log probability\n",
    "    scores = [scores[i] for i in sorted_indices]\n",
    "    codes = [codes[i] for i in sorted_indices]\n",
    "    return codes, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f08892dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, scores = best_of_n_logprob(prompts[0], n=10, temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa34a1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03816219730931771, -0.046072972968882075, -0.04632711074250333, -0.0723968563913914, -0.0797575224169243, -0.09274121795797267, -0.09437068446480579, -0.12788232105058547, -0.12788232105058547, -0.14468262731788623]\n",
      "def remove_Occ(s, char):\n",
      "    first_occ = s.find(char)\n",
      "    last_occ = s.rfind(char)\n",
      "    if first_occ != -1:\n",
      "        s = s[:first_occ] + s[first_occ+1:]\n",
      "    if last_occ != -1:\n",
      "        s = s[:last_occ] + s[last_occ+1:]\n",
      "    return s\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(codes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9f331e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution_results = execute_tests(codes, mbpp[0]['test_list'])\n",
    "[a['result'] for a in execution_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f8699-5fb7-4f8a-b027-39dacd936144",
   "metadata": {},
   "source": [
    "### MBR\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{y} &= \\arg \\min_{y \\in \\mathcal{Y}} R(y) = \\arg \\max_{y \\in \\mathcal{Y}} G(y) \\\\\n",
    "R(y) &= \\sum_{y' \\in \\mathcal{Y_e}} \\ell(y, y') = 1 - G(y)\n",
    "\\end{align*}\n",
    "\n",
    "In this section, we consider minimum Bayes risk. We consider two choices of gain function: edit similarity or execution equivalence [(Shi et al.)](https://arxiv.org/abs/2204.11454)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c42faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PairwiseMetric = Callable[[str, str], float]\n",
    "\n",
    "def mbr(prompt: str, metric_fn: PairwiseMetric, **generate_kwargs) -> tuple[list[str], list[float]]:\n",
    "    '''\n",
    "    Runs MBR decoding with custom pairwise metric\n",
    "    '''\n",
    "    codes, response = generate_code(prompt, **generate_kwargs)\n",
    "    pairwise_scores = np.zeros((len(codes), len(codes)))\n",
    "    for i1, code1 in enumerate(codes):\n",
    "        for i2, code2 in enumerate(codes):\n",
    "            if i1 <= i2:\n",
    "                sim = metric_fn(code1, code2)\n",
    "                pairwise_scores[i1, i2] = sim\n",
    "                pairwise_scores[i2, i1] = sim\n",
    "    gains = pairwise_scores.mean(axis=-1)\n",
    "    sorted_indices = np.argsort(gains)[::-1]\n",
    "\n",
    "    gains = [float(gains[i]) for i in sorted_indices]\n",
    "    codes = [codes[i] for i in sorted_indices]\n",
    "    return codes, gains\n",
    "\n",
    "def edit_sim(code1, code2):\n",
    "    edit_distance = jellyfish.levenshtein_distance(code1, code2)\n",
    "    return 1 - edit_distance / max(len(code1), len(code2))\n",
    "\n",
    "def make_exec_metric(test_list):\n",
    "    '''\n",
    "    Make pairwise similarity metric for specific example based on that example's test cases\n",
    "    '''\n",
    "    # extract the calls to this example's function\n",
    "    test_func_calls = extract_func_calls(test_list)\n",
    "    \n",
    "    def exec_sim(code1, code2):\n",
    "        '''\n",
    "        Runs code1 and code2 on test_func_calls (from the closure of this function)\n",
    "        Returns proportion of func calls with same execution result\n",
    "        '''\n",
    "        result1, result2 = execute_codes([code1, code2], test_func_calls)\n",
    "        n_same = 0\n",
    "        for r1, r2 in zip(result1, result2):\n",
    "            if isinstance(r1, Exception) or isinstance(r2, Exception):\n",
    "                continue\n",
    "            if r1 == r2:\n",
    "                n_same += 1\n",
    "        similarity = n_same / len(test_func_calls)\n",
    "        return similarity\n",
    "    return exec_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6db547e0-a79a-48bb-9f0a-f038c0c8bcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6921033971312858, 0.6921033971312858, 0.6415651261880847, 0.6163579617256353, 0.6122981366459628, 0.6076617274005611, 0.6027653814183951, 0.5051370324289156, 0.4704761904761905, 0.42899317679638865]\n",
      "def remove_Occ(s, char):\n",
      "    first_occ = s.find(char)\n",
      "    last_occ = s.rfind(char)\n",
      "    \n",
      "    if first_occ != -1:\n",
      "        s = s[:first_occ] + s[first_occ+1:]\n",
      "    if last_occ != -1:\n",
      "        s = s[:last_occ] + s[last_occ+1:]\n",
      "    \n",
      "    return s\n"
     ]
    }
   ],
   "source": [
    "# MBR-edit-sim\n",
    "codes, gains = mbr(prompts[0], edit_sim, n=10, temperature=0.9)\n",
    "print(gains)\n",
    "print(codes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1dccfed1-77ef-4d8e-b5df-5820c532d047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution_results = execute_tests(codes, mbpp[0]['test_list'])\n",
    "[a['result'] for a in execution_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b57ea1f2-d603-49d4-8822-7abb4641ea33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.36666666666666664, 0.36666666666666664, 0.36666666666666664, 0.29999999999999993]\n",
      "def remove_Occ(s, char):\n",
      "    first_index = s.find(char)\n",
      "    last_index = s.rfind(char)\n",
      "    \n",
      "    if first_index != -1:\n",
      "        s = s[:first_index] + s[first_index+1:]\n",
      "    if last_index != -1:\n",
      "        s = s[:last_index] + s[last_index+1:]\n",
      "    \n",
      "    return s\n"
     ]
    }
   ],
   "source": [
    "# MBR-exec\n",
    "codes, gains = mbr(prompts[0], make_exec_metric(mbpp[0]['test_list']), n=10, temperature=0.9)\n",
    "print(gains)\n",
    "print(codes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd0e813c-1c26-46a7-8b43-b4f35443aa48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution_results = execute_tests(codes, mbpp[0]['test_list'])\n",
    "[a['result'] for a in execution_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79633f3b-9f05-4d5d-b6f3-45c9d947bc71",
   "metadata": {},
   "source": [
    "### Self-repair\n",
    "\n",
    "Self-repair (or self-debugging) is a refinement strategy in which the code LLM iteratively repairs its own previously-generated code based on compiler or execution error messages. Here, the __initial generation__ is the code LLM's initial code samples and __feedback__ is derived from the code execution environment. Finally, the model __refines__ its generation by (optionally) explaining the previous turn's error and writing an improved piece of code.\n",
    "\n",
    "\\begin{align*}\n",
    "    y^{(0)}&\\sim g_0(y|x) \\tag{initial generation}\\\\\n",
    "    z^{(t)}&\\sim h(z|x,y^{(<t)},z^{(<t)}) \\tag{feedback}\\\\\n",
    "    y^{(t)}&\\sim g(y|x,y^{(<t)},z^{(\\leq t)}) \\tag{refinement}\n",
    "\\end{align*}\n",
    "\n",
    "Self-repair has been shown to be effective for a variety of code generation tasks [(Chen et al.)](https://arxiv.org/abs/2304.05128). However, for more challenging tasks, it has been found to have some limitations and may not always be more effective than best-of-N [(Olausson et al.)](https://arxiv.org/abs/2306.09896)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b733c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example repair prompt from Olausson et al (https://arxiv.org/abs/2306.09896).\n",
    "repair_template = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful programming assistant and an expert Python programmer. \"\n",
    "                   \"You are helping a user write a program to solve a problem. \"\n",
    "                   \"The user has written some code, but it has some errors and is not passing the tests. \"\n",
    "                   \"You will help the user by first giving a concise (at most 2-3 sentences) textual explanation \"\n",
    "                   \"of what is wrong with the code. After you have pointed out what is wrong with the code, \"\n",
    "                   \"you will then generate a fixed version of the program. \"\n",
    "                   \"Put your fixed program within code delimiters, for example: ```\\n# YOUR CODE HERE\\n```.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"### Problem\\n\"\n",
    "                   \"{problem}\\n\\n\"\n",
    "                   \"### Incorrect Code:\\n\"\n",
    "                   \"{code}\\n\\n\"\n",
    "                   \"###Error\\n\"\n",
    "                   \"{error}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def run_repair(code: str, problem: str, error_msg: str, **generate_kwargs) -> str:\n",
    "    '''\n",
    "    Runs a single iteration of refinement given the previous round's code's error message\n",
    "    '''\n",
    "    repair_prompt = copy.deepcopy(repair_template)\n",
    "    repair_prompt[-1]['content'] = repair_prompt[-1]['content'].format(problem=problem, code=code, error=error_msg)\n",
    "\n",
    "    new_code, response = generate_code(repair_prompt, **generate_kwargs)\n",
    "    return new_code, response\n",
    "\n",
    "\n",
    "def self_repair(prompt: str, tests: list[str], max_rounds: int, **generate_kwargs) -> list[str]:\n",
    "    '''\n",
    "    Runs generation with self-repair for a given prompt and tests.\n",
    "    '''\n",
    "    problem = prompt.rsplit(\"\\n\", 1)[0] # remove the additional instruction at the end of the prompt\n",
    "    codes, response = generate_code(prompt, **generate_kwargs)\n",
    "\n",
    "    execution_results = execute_codes(codes, tests)\n",
    "\n",
    "    refine_history = []\n",
    "\n",
    "    refine_history.append((codes, response, copy.deepcopy(execution_results)))\n",
    "    for round in range(max_rounds):\n",
    "        refined_codes = []\n",
    "        refined_responses = []\n",
    "        for code, exec_result in zip(codes, execution_results):\n",
    "            if exec_result['passed']:\n",
    "                refined_codes.append(None)\n",
    "                refined_responses.append(None)\n",
    "                continue\n",
    "\n",
    "            refined_code, refined_response = run_repair(code, exec_result['result'], n=1)\n",
    "            refined_codes.append(refined_code[0])\n",
    "            refined_responses.append(refined_response)\n",
    "        \n",
    "        execution_results = execute_codes(refined_codes, tests)\n",
    "        codes = refined_code\n",
    "        refine_history.append((refined_codes, refined_responses, copy.deepcopy(execution_results)))\n",
    "\n",
    "    return refine_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbe8df7-7253-45b3-8df2-c25e5722cdaf",
   "metadata": {},
   "source": [
    "Observe that the model's earlier initial generation did not pass the tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e1cbe55d-2118-4a98-a8e7-57a4bd879fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code:\n",
      "def remove_Occ(s, char):\n",
      "    first_occ = s.find(char)\n",
      "    last_occ = s.rfind(char)\n",
      "    \n",
      "    if first_occ != -1:\n",
      "        s = s[:first_occ] + s[first_occ+1:]\n",
      "    if last_occ != -1:\n",
      "        s = s[:last_occ] + s[last_occ+1:]\n",
      "    \n",
      "    return s\n",
      "\n",
      "Result:\n",
      "failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" \n"
     ]
    }
   ],
   "source": [
    "print(\"Code:\\n\" + codes[0] + \"\\n\")\n",
    "print(\"Result:\\n\" + execution_results[0]['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af995509-80ee-438a-9dc7-c5770dc16d7a",
   "metadata": {},
   "source": [
    "Now, let's try an iteration of refinement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6b31cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = prompts[0].rsplit(\"\\n\", 1)[0]\n",
    "repair_code, repair_response = run_repair(codes[0], problem, execution_results[0]['result'], n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27a4fc-7a6f-4223-b38b-0186f1df6103",
   "metadata": {},
   "source": [
    "The new result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ef53b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'task_id': None, 'passed': True, 'result': 'passed', 'completion_id': None}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_tests(repair_code, mbpp[0]['test_list'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bd79f9-45a4-427c-a88c-f19c98c33aa5",
   "metadata": {},
   "source": [
    "And the LLM's reasoning for its refinement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34ba95a0-4375-4b2e-8c74-582eac899f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The issue with the code is that it removes the last occurrence of the character before removing the first occurrence. This causes the removal of the first occurrence to shift the index of the last occurrence, resulting in the incorrect output. To fix this, you should remove the last occurrence first and then the first occurrence.\n",
      "\n",
      "```python\n",
      "def remove_Occ(s, char):\n",
      "    last_occ = s.rfind(char)\n",
      "    first_occ = s.find(char)\n",
      "    \n",
      "    if last_occ != -1:\n",
      "        s = s[:last_occ] + s[last_occ+1:]\n",
      "    if first_occ != -1:\n",
      "        s = s[:first_occ] + s[first_occ+1:]\n",
      "    \n",
      "    return s\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(repair_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5a0d9-04ef-4280-a84c-ae77b93fa7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
