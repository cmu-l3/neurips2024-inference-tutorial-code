{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6c6ddd61b5da47c485bb76a58b2a9209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47a095fb594248759a3f1cbcdf4ea9e5",
              "IPY_MODEL_2812cc4f57b4486db4bc9a683aa85344",
              "IPY_MODEL_743fc252b9d34b67a085e8ceb1e51157"
            ],
            "layout": "IPY_MODEL_956b75b79993422b860c0aa7c4bc603e"
          }
        },
        "47a095fb594248759a3f1cbcdf4ea9e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7ea3a3942a14bd48d1241ad778ad603",
            "placeholder": "​",
            "style": "IPY_MODEL_1ce09408fd5c4cccbde4e5ffa6bc9345",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2812cc4f57b4486db4bc9a683aa85344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe4763deef8d463081c85a63bdbc32d0",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_774006a526b54d759f09346200382249",
            "value": 2
          }
        },
        "743fc252b9d34b67a085e8ceb1e51157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09a197c4173c4774bd3eef29dc5e713a",
            "placeholder": "​",
            "style": "IPY_MODEL_de34b455ae114abeb0e55d2a585277f3",
            "value": " 2/2 [00:30&lt;00:00, 14.09s/it]"
          }
        },
        "956b75b79993422b860c0aa7c4bc603e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7ea3a3942a14bd48d1241ad778ad603": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ce09408fd5c4cccbde4e5ffa6bc9345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe4763deef8d463081c85a63bdbc32d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "774006a526b54d759f09346200382249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "09a197c4173c4774bd3eef29dc5e713a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de34b455ae114abeb0e55d2a585277f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes transformers accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AeIuOqDp4rn",
        "outputId": "579472d6-21db-45df-e826-29ed743d95c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "6c6ddd61b5da47c485bb76a58b2a9209",
            "47a095fb594248759a3f1cbcdf4ea9e5",
            "2812cc4f57b4486db4bc9a683aa85344",
            "743fc252b9d34b67a085e8ceb1e51157",
            "956b75b79993422b860c0aa7c4bc603e",
            "f7ea3a3942a14bd48d1241ad778ad603",
            "1ce09408fd5c4cccbde4e5ffa6bc9345",
            "fe4763deef8d463081c85a63bdbc32d0",
            "774006a526b54d759f09346200382249",
            "09a197c4173c4774bd3eef29dc5e713a",
            "de34b455ae114abeb0e55d2a585277f3"
          ]
        },
        "id": "bPIHWdgQPE98",
        "outputId": "0337c8a5-ae17-4c2c-f1ca-4e27d19aba17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA GPU is available\n",
            "Device name: Tesla T4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c6ddd61b5da47c485bb76a58b2a9209"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pprint\n",
        "import torch\n",
        "import os\n",
        "from torch.distributions import Categorical\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, set_seed, BitsAndBytesConfig\n",
        "\n",
        "# Default device is CPU\n",
        "device = torch.device('cpu')\n",
        "\n",
        "# Check if CUDA GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print('CUDA GPU is available')\n",
        "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    try:\n",
        "        import torch_xla.core.xla_model as xm\n",
        "    except ImportError:\n",
        "        # Install torch_xla for TPU support\n",
        "        print('Installing torch_xla for TPU support...')\n",
        "        !pip install --quiet torch_xla torch_xla-core\n",
        "        import torch_xla.core.xla_model as xm\n",
        "    device = xm.xla_device()\n",
        "    print(f\"Using TPU device: {device}\")\n",
        "\n",
        "target_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "auxilary_model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "# Load models and tokenizer\n",
        "compute_dtype = torch.float16\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        llm_int8_threshold=6.0,\n",
        "        llm_int8_has_fp16_weight=False,\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "model_args = {\n",
        "    \"torch_dtype\": compute_dtype,\n",
        "    \"quantization_config\": quantization_config,\n",
        "    \"device_map\": \"auto\"\n",
        "}\n",
        "\n",
        "target_model = AutoModelForCausalLM.from_pretrained(target_model_name, **model_args)\n",
        "auxilary_model = AutoModelForCausalLM.from_pretrained(auxilary_model_name, **model_args)\n",
        "tokenizer = AutoTokenizer.from_pretrained(target_model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def sample_decode(model, tokenizer, input_ids: torch.Tensor, max_new_tokens: int, temperature: float = 1.0) -> torch.Tensor:\n",
        "    log_zero = -1e4\n",
        "\n",
        "    # Initialize generated tokens with the input prompt\n",
        "    generated_ids = input_ids\n",
        "    finished_sequences = torch.zeros(input_ids.shape[0], dtype=torch.bool, device=model.device)\n",
        "    log_probs = []\n",
        "\n",
        "    # Iteratively generate tokens using greedy decoding\n",
        "    for token_idx in range(max_new_tokens):\n",
        "        # Filter out finished sequences\n",
        "        active_indices = torch.nonzero(~finished_sequences).squeeze(-1)\n",
        "        if len(active_indices) == 0:\n",
        "            break\n",
        "\n",
        "        # Get model outputs for active sequences\n",
        "        active_input_ids = generated_ids[active_indices]\n",
        "        outputs = model(input_ids=active_input_ids)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Get the last token logits and apply argmax to select the next token\n",
        "        next_token_logits = logits[:, -1, :] / temperature\n",
        "        next_token_log_probs = torch.nn.functional.log_softmax(next_token_logits, dim=-1)\n",
        "        next_token_id = Categorical(logits=next_token_log_probs).sample()\n",
        "        # next_token_log_prob, next_token_id = next_token_log_probs.max(dim=-1)\n",
        "\n",
        "        # Save log next-token distribution for each sequence in batch; inactivate sequences produce <pad> token with probability 1\n",
        "        curr_log_probs = torch.full((input_ids.shape[0], len(tokenizer)), log_zero, dtype=next_token_log_probs.dtype, device=model.device)\n",
        "        curr_log_probs[:, tokenizer.pad_token_id] = 0.0\n",
        "        curr_log_probs[active_indices] = next_token_log_probs\n",
        "        log_probs.append(curr_log_probs)\n",
        "\n",
        "        # Update finished sequences and add padding if necessary\n",
        "        finished_sequences[active_indices] |= (next_token_id == tokenizer.eos_token_id)\n",
        "\n",
        "        # Create a tensor for the next tokens to append to all sequences\n",
        "        new_tokens = torch.full((generated_ids.shape[0], 1), tokenizer.pad_token_id, dtype=torch.long, device=model.device)\n",
        "        new_tokens[active_indices] = next_token_id.unsqueeze(-1)\n",
        "\n",
        "        # Append the next token to the generated sequence\n",
        "        generated_ids = torch.cat([generated_ids, new_tokens], dim=-1)\n",
        "\n",
        "    return generated_ids, log_probs"
      ],
      "metadata": {
        "id": "KQSne5e5P4jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def specualative_decode(target_model,   , tokenizer, input_ids: torch.Tensor,\n",
        "                        max_new_tokens: int, num_speculated: int = 5, temperature: float = 1.0) -> torch.Tensor:\n",
        "    # Initialize generated tokens with the input prompt\n",
        "    generated_ids = input_ids\n",
        "    max_length = input_ids.shape[1] + max_new_tokens\n",
        "\n",
        "    while generated_ids.shape[1] < max_length:\n",
        "        tokens_remaining = max_length - generated_ids.shape[1]\n",
        "        speculation_size = min(num_speculated, tokens_remaining - 1)\n",
        "\n",
        "        if speculation_size > 0:\n",
        "\n",
        "            # Generate speculative tokens\n",
        "            speculated_ids, speculated_log_probs = sample_decode(auxilary_model, tokenizer, generated_ids, speculation_size, temperature)\n",
        "            speculation_size = speculated_ids.shape[1] - generated_ids.shape[1]\n",
        "            speculated_token_ids = speculated_ids[:, -speculation_size:]\n",
        "            speculated_log_probs = torch.stack(speculated_log_probs, dim=1).squeeze(0)\n",
        "\n",
        "            # Verify all speculative tokens in one forward pass\n",
        "            outputs = target_model(input_ids=speculated_ids)\n",
        "            target_logits = outputs.logits[:, -(speculation_size + 1):, :].squeeze(0) / temperature\n",
        "            target_log_probs = torch.nn.functional.log_softmax(target_logits, dim=-1)\n",
        "\n",
        "            # Compare log-likelihood ratios of target and speculative tokens; use unifrorm (0, 1) distribution to decide acceptance\n",
        "            log_likelihood_ratios = target_log_probs[:-1].gather(1, speculated_token_ids.view(-1,1)) \\\n",
        "                                    - speculated_log_probs.gather(1, speculated_token_ids.view(-1,1))\n",
        "            uniform_log_probs = torch.log(torch.rand_like(log_likelihood_ratios))\n",
        "            rejected_indexes = torch.nonzero((log_likelihood_ratios <= uniform_log_probs).squeeze(-1))\n",
        "\n",
        "            if len(rejected_indexes) > 0:\n",
        "                # Some speculative tokens are rejected, truncate the accepted tokens\n",
        "                rejected_token_idx = rejected_indexes[0]\n",
        "                accepted_ids = speculated_token_ids[:, :rejected_token_idx]\n",
        "\n",
        "                # Sample the next token from the adjusted distribution\n",
        "                adjusted_distribution = torch.clamp(\n",
        "                    torch.exp(target_log_probs[rejected_token_idx]) - torch.exp(speculated_log_probs[rejected_token_idx]),\n",
        "                    min=0\n",
        "                )\n",
        "                adjusted_distribution = torch.div(adjusted_distribution, adjusted_distribution.sum())\n",
        "                next_token_id = Categorical(probs=adjusted_distribution).sample()\n",
        "\n",
        "            else:\n",
        "                # All speculative tokens are accepted, sample the next token from target model\n",
        "                accepted_ids = speculated_token_ids\n",
        "                if accepted_ids[0, -1].item() != tokenizer.eos_token_id:\n",
        "                    next_token_id = Categorical(logits=target_logits[[-1]]).sample()\n",
        "\n",
        "            # Append the accepted tokens to the generated sequence\n",
        "            if accepted_ids.numel() == 0 or (accepted_ids.numel() > 0  and accepted_ids[0, -1].item() != tokenizer.eos_token_id):\n",
        "                new_tokens = torch.cat([accepted_ids, next_token_id.unsqueeze(-1)], dim=-1)\n",
        "            else:\n",
        "                new_tokens = accepted_ids\n",
        "\n",
        "            generated_ids = torch.cat([generated_ids, new_tokens], dim=-1)\n",
        "\n",
        "        else:\n",
        "            # If no speculation is performed, use the target model for generation\n",
        "            outputs = target_model(input_ids=generated_ids)\n",
        "            target_logits = outputs.logits[:, -1, :].squeeze(0) / temperature\n",
        "            target_log_probs = torch.nn.functional.log_softmax(target_logits, dim=-1).unsqueeze(0)\n",
        "            next_token_id = Categorical(logits=target_log_probs).sample()\n",
        "            generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(0)], dim=-1)\n",
        "\n",
        "        if generated_ids[0, -1] == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    return generated_ids"
      ],
      "metadata": {
        "id": "CNxmC_zfP9TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    [\n",
        "        {'role': 'system', 'content': 'You are an algebra assistant. The user will ask you math questions and you will solve them.'},\n",
        "        {'role': 'user', 'content': \"Peter purchased 20 popsicles at $0.25 each. He also purchased 2730244 ice cream bars at $0.50 each. How much did he pay in total in dollars?\"},\n",
        "    ],\n",
        "]\n",
        "max_new_tokens = 120\n",
        "temperature = 0.001\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(messages)\n",
        "for input_ids, message in zip(inputs, messages):\n",
        "    input_ids = torch.tensor(input_ids, device=target_model.device).unsqueeze(0)\n",
        "\n",
        "    set_seed(42)\n",
        "    speculative_ids = specualative_decode(target_model, auxilary_model, tokenizer, input_ids, max_new_tokens, temperature=temperature)\n",
        "\n",
        "    set_seed(42)\n",
        "    sampled_ids, log_probs = sample_decode(target_model, tokenizer, input_ids, max_new_tokens, temperature=temperature)\n",
        "\n",
        "    if torch.equal(speculative_ids, sampled_ids):\n",
        "        print(\"The outputs match!\")\n",
        "    else:\n",
        "        print(\"The outputs do not match.\")\n",
        "\n",
        "    speculative_text = tokenizer.batch_decode(speculative_ids, skip_special_tokens=True)\n",
        "    sampled_text = tokenizer.batch_decode(sampled_ids, skip_special_tokens=True)\n",
        "\n",
        "    pprint.pprint({\"Prompt\": message, \"Speculative\": speculative_text, \"Sampled\": sampled_text})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBRSXdWnP-61",
        "outputId": "ad5d1da8-c00c-4e4a-b073-ffa43d6afcd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The outputs match!\n",
            "{'Prompt': [{'content': 'You are an algebra assistant. The user will ask you '\n",
            "                        'math questions and you will solve them.',\n",
            "             'role': 'system'},\n",
            "            {'content': 'Peter purchased 20 popsicles at $0.25 each. He also '\n",
            "                        'purchased 2730244 ice cream bars at $0.50 each. How '\n",
            "                        'much did he pay in total in dollars?',\n",
            "             'role': 'user'}],\n",
            " 'Sampled': ['system\\n'\n",
            "             '\\n'\n",
            "             'Cutting Knowledge Date: December 2023\\n'\n",
            "             'Today Date: 07 Nov 2024\\n'\n",
            "             '\\n'\n",
            "             'You are an algebra assistant. The user will ask you math '\n",
            "             'questions and you will solve them.user\\n'\n",
            "             '\\n'\n",
            "             'Peter purchased 20 popsicles at $0.25 each. He also purchased '\n",
            "             '2730244 ice cream bars at $0.50 each. How much did he pay in '\n",
            "             'total in dollars?assistant\\n'\n",
            "             '\\n'\n",
            "             'To find the total amount Peter paid, we need to calculate the '\n",
            "             'cost of the popsicles and the ice cream bars separately and then '\n",
            "             'add them together.\\n'\n",
            "             '\\n'\n",
            "             'Cost of popsicles: 20 popsicles * $0.25/popsicle = $5\\n'\n",
            "             'Cost of ice cream bars: 2730244 ice cream bars * $0.50/ice cream '\n",
            "             'bar = $1365122\\n'\n",
            "             '\\n'\n",
            "             'Total cost: $5 + $1365122 = $1365127\\n'\n",
            "             '\\n'\n",
            "             'So, Peter paid a total of $1365127.'],\n",
            " 'Speculative': ['system\\n'\n",
            "                 '\\n'\n",
            "                 'Cutting Knowledge Date: December 2023\\n'\n",
            "                 'Today Date: 07 Nov 2024\\n'\n",
            "                 '\\n'\n",
            "                 'You are an algebra assistant. The user will ask you math '\n",
            "                 'questions and you will solve them.user\\n'\n",
            "                 '\\n'\n",
            "                 'Peter purchased 20 popsicles at $0.25 each. He also '\n",
            "                 'purchased 2730244 ice cream bars at $0.50 each. How much did '\n",
            "                 'he pay in total in dollars?assistant\\n'\n",
            "                 '\\n'\n",
            "                 'To find the total amount Peter paid, we need to calculate '\n",
            "                 'the cost of the popsicles and the ice cream bars separately '\n",
            "                 'and then add them together.\\n'\n",
            "                 '\\n'\n",
            "                 'Cost of popsicles: 20 popsicles * $0.25/popsicle = $5\\n'\n",
            "                 'Cost of ice cream bars: 2730244 ice cream bars * $0.50/ice '\n",
            "                 'cream bar = $1365122\\n'\n",
            "                 '\\n'\n",
            "                 'Total cost: $5 + $1365122 = $1365127\\n'\n",
            "                 '\\n'\n",
            "                 'So, Peter paid a total of $1365127.']}\n"
          ]
        }
      ]
    }
  ]
}